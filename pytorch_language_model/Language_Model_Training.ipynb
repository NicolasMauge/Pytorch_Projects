{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_Model_Training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolasMauge/learning_projects/blob/master/pytorch_language_model/Language_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aVl77kMtkavO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set the environment"
      ]
    },
    {
      "metadata": {
        "id": "NvKhlK_Wieip",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation of PyTorch and the needed repositories"
      ]
    },
    {
      "metadata": {
        "id": "puLh_if3EVc6",
        "colab_type": "code",
        "outputId": "28318210-1c98-47db-c5d1-d01b9b2cb542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NicolasMauge/utils_google_colab\n",
        "!. utils_google_colab/colab_init.sh\n",
        "!git clone https://github.com/NicolasMauge/learning_projects"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'utils_google_colab'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 72 (delta 1), reused 6 (delta 1), pack-reused 66\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n",
            "Install of the PyTorch packages\n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58772000 @  0x7fb49b21a2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Install of the PyDrive package\n",
            "Install of the kaggle package\n",
            "Cloning into 'learning_projects'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 206 (delta 54), reused 96 (delta 29), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (206/206), 258.31 KiB | 4.87 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O7OqrSg_xA1w",
        "colab_type": "code",
        "outputId": "295d0996-3a8d-49d5-f2dc-5b77850aa0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "!cd learning_projects && git fetch --all && git reset --hard origin/master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching origin\n",
            "HEAD is now at 4564b05 index -> self.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gp23jn3cF94p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append('utils_google_colab')\n",
        "sys.path.append('learning_projects/pytorch_language_model')\n",
        "from colab_utils import download\n",
        "from colab_utils import upload\n",
        "import get_data\n",
        "from utils.utils import load_vocabulary\n",
        "from models.models import get_language_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L198_AcKivc3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the text corpus and the dictionary itos"
      ]
    },
    {
      "metadata": {
        "id": "zA087fWbtfxs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "'wiki_text.csv' and 'vocab_itos.pkl' have been produced by the repository 'wikipedia_extract'"
      ]
    },
    {
      "metadata": {
        "id": "dezVKvAqF_c5",
        "colab_type": "code",
        "outputId": "5e4cd241-86b0-4b97-8b3b-90b47873ac1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "split = False\n",
        "if not Path(\"wiki_text.csv\").is_file():\n",
        "    download(\"wiki_text.csv\")\n",
        "    split = True\n",
        "    \n",
        "\n",
        "if not Path(\"vocab_itos.pkl\").is_file():\n",
        "    download(\"vocab_itos.pkl\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 16%.\n",
            "Download 32%.\n",
            "Download 49%.\n",
            "Download 65%.\n",
            "Download 82%.\n",
            "Download 98%.\n",
            "Download 100%.\n",
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XzNy3HKSHOw8",
        "colab_type": "code",
        "outputId": "da4c02fd-b5c6-4dba-f7a2-bfb99bd7b413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary = load_vocabulary(\"vocab_itos.pkl\")\n",
        "n_words = len(vocabulary)\n",
        "print(f\"Vocabulary: {n_words} words\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: 80002 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7QUVZjitWJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set model"
      ]
    },
    {
      "metadata": {
        "id": "RgazRmTUuKhm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model defined in models.py (program issued from fastai) is an AWD-LSTM"
      ]
    },
    {
      "metadata": {
        "id": "zOZ1l3zgHyBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "emb_size = 400\n",
        "nhid = 1150\n",
        "model = get_language_model(n_words, emb_size, nhid, 3, 1)\n",
        "if device == 'cuda':\n",
        "\tmodel = torch.nn.DataParallel(model)\n",
        "\tcudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SEGhX_UVCoaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, weight_decay=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qPVSpCbvHhX",
        "colab_type": "code",
        "outputId": "4d80f956-b5bf-4f2c-c7f6-62fcff196af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov 25 07:33:55 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    69W / 149W |    553MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z3dayh8HCsLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model(filename, to_google_drive=False):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    if to_google_drive:     \n",
        "        upload(filename)\n",
        "\n",
        "def load_model(filename, from_google_drive=False):\n",
        "    if from_google_drive:     \n",
        "        download(filename)\n",
        "        \n",
        "    model.load_state_dict(torch.load(filename))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNaYs3vEvX15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "CNMvLMNYvgx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's split the data in train / valid"
      ]
    },
    {
      "metadata": {
        "id": "ZMRc43lvvfXW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# à lancer la première fois\n",
        "if split:\n",
        "    get_data.get_data.split(\"wiki_text.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDwSjB5E7D6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIJIGTvKCHsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ]
    },
    {
      "metadata": {
        "id": "TdU7HvbxICHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# on colab, n batch is set on 8 because > 10 can't be processed due to gpu memory error\n",
        "filenames = {\"train\":\"wiki_text_train.csv\", \"test\":\"wiki_text_test.csv\", \"valid\":\"wiki_text_valid.csv\"}\n",
        "data_class = get_data.get_data(70, filenames, n_batch=10, phase=\"train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xzL4fl3sCK3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some tests"
      ]
    },
    {
      "metadata": {
        "id": "w3pkpnRDCQIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_class.set_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eq50m5PaCT4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iter_d = iter(data_class)\n",
        "val1, t1 = next(iter_d)\n",
        "val2, t2 = next(iter_d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycIL6b7rCWI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return vocabulary[x]\n",
        "\n",
        "print(f\"input: {np.vectorize(f)(val1)}\\n\")\n",
        "print(f\"target: {np.vectorize(f)(t1)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84CXixftx3WU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one step\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer.zero_grad()\n",
        "if device=='cuda':\n",
        "    input_train = val1.cuda()\n",
        "    target_train = t1.cuda()\n",
        "    \n",
        "    decoded, _, _ = model(input_train)\n",
        "    \n",
        "    #print(f\"input: {np.vectorize(f)(decoded.cpu().detach().numpy())}\\n\")\n",
        "    #loss =  F.cross_entropy(decoded, target_train)\n",
        "    print(decoded)\n",
        "    print(decoded.shape)\n",
        "    \n",
        "    print(target_train)\n",
        "    print(target_train.shape)\n",
        "    loss = criterion(decoded, target_train)\n",
        "    loss.backward()\n",
        "        \n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vl_x95PeC29l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "IN5Tup4zCSoW",
        "colab_type": "code",
        "outputId": "04d064bf-a76f-41dc-c420-f14e229ad39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "load_model(\"model_last15.pth\", from_google_drive=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 35%.\n",
            "Download 70%.\n",
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-yRM3JJzIzHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_class.set_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iIP90v74JDTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "last_index=0\n",
        "list_loss=[]\n",
        "num=0\n",
        "with tqdm.tqdm(total=data_class.data.shape[0], position=0) as pbar:\n",
        "    total_loss = 0\n",
        "    for index, (input_train, target_train) in enumerate(data_class):\n",
        "        model.zero_grad()\n",
        "        if device=='cuda':\n",
        "            input_train = input_train.cuda()\n",
        "            target_train = target_train.cuda()\n",
        "\n",
        "        decoded, raw_outputs, outputs = model(input_train)\n",
        "        loss =  F.cross_entropy(decoded, target_train)\n",
        "        loss.backward()\n",
        "        list_loss.append(float(loss))\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        pbar.update(data_class.seq_len)\n",
        "        if len(list_loss)> 100:\n",
        "            moy = np.mean(list_loss[-100:])\n",
        "            pbar.set_description(f\"Loss: {moy}\")\n",
        "        \n",
        "        \n",
        "        if data_class.index//100000 > last_index:\n",
        "            last_index = data_class.index//100000 \n",
        "            num += 1\n",
        "            save_model(\"model_last\"+str(num)+\".pth\", to_google_drive=True)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9hJb1gAjMJCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ]
    },
    {
      "metadata": {
        "id": "Kp5bF5f8MNR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model does not converge quickly:\n",
        "- retrain the model with no punctuation / accents / less vocabulary \n",
        "- do transfer learning with this new model to add the punctuation, accents and 80 000 words\n",
        "(difficulty: create a mapping between the embedding with no punctuation, etc. and the new model, duplicate weights for the differents words with accents, etc.)"
      ]
    }
  ]
}